{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.special\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, ElasticNetCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.base import clone\n",
    "import joblib\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.model_selection import StratifiedGroupKFold, GroupKFold, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the High Level Parameters for the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_budget = 60 # time budget for auto-ml in seconds (advisable at least 120)\n",
    "verbose = 0 # verbosity of auto-ml\n",
    "n_splits = 5 # cross-fitting and cross-validation splits\n",
    "cfit = False\n",
    "data = '401k' # which dataset, one of {'401k', 'criteo', 'welfare', 'poverty', 'star'}\n",
    "plot = True # whether to plot results\n",
    "xfeat = 'inc' # feature to use as x axis in plotting, e.g. for criteo 'f1', for 401k 'inc', for welfare 'polviews'\n",
    "# Formula for the BLP of CATE regression.\n",
    "blp_formula = 'np.log(inc)' # e.g. 'f1' for criteo, np.log(inc)' for 401k, 'C(polviews)' for the welfare case.\n",
    "blp_formula2 = 'np.log(inc) + np.power(np.log(inc), 2) + np.power(np.log(inc), 3) + np.power(np.log(inc), 4)'\n",
    "hetero_feats = 'all' # list of subset of features to be used for CATE model or the string 'all' for everything\n",
    "cov_clip = .01 # clipping of treatment variance p(x)*(1-p(x)), whenever used in inverse propensities\n",
    "binary_y = False\n",
    "random_seed = 12\n",
    "\n",
    "## For semi-synthetic data generation\n",
    "semi_synth = False # Whether true outcome y should be replaced by a fake outcome from a known CEF\n",
    "simple_synth = True # Whether the true CEF of the fake y should be simple or fitted from data\n",
    "max_depth = 2 # max depth of random forest during for semi-synthetic model fitting\n",
    "scale = .2 # magnitude of noise in semi-synthetic data\n",
    "def simple_true_cef(D, X): # simple CEF of the outcome for semi-synthetic data\n",
    "    return .5 * np.array(X)[:, 1] * D + np.array(X)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Criteo dataset can be downloaded from here:\n",
    "https://www.kaggle.com/code/hughhuyton/criteo-uplift-modelling/input\n",
    "This file should be downloaded and stored in the same folder as the notebook with the name `criteo-uplift-v2.1.csv`.\n",
    "\n",
    "The welfare dataset can be downloaded from here:\n",
    "https://github.com/gsbDBI/ExperimentData/blob/master/Welfare/ProcessedData/welfarenolabel3.csv\n",
    "This file should be downloaded and stored in the same folder as the notebook with the name `welfarenolabel3.csv`. And is drawn from the analysis in this paper: [Green and Kern, 2012, Modeling Heterogeneous Treatment Effects in Survey Experiments with Bayesian Additive Regression Trees](https://github.com/gsbDBI/ExperimentData/blob/master/Welfare/Green%20and%20Kern%20BART.pdf)\n",
    "\n",
    "The 401k dataset is downloaded from the source by the code and no need to further download anything:\n",
    "https://raw.githubusercontent.com/CausalAIBook/MetricsMLNotebooks/main/data/401k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import fetch_data_generator\n",
    "\n",
    "get_data, abtest, true_cef, true_cate = fetch_data_generator(data=data, semi_synth=semi_synth,\n",
    "                                                             simple_synth=simple_synth,\n",
    "                                                             scale=scale, true_f=simple_true_cef,\n",
    "                                                             max_depth=max_depth)\n",
    "X, D, y, groups = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if semi_synth:\n",
    "    true_ate = np.mean(true_cate(X))\n",
    "    print(f'True ATE: {true_ate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(cate, preds):\n",
    "    return np.sqrt(np.mean((cate - preds)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "We now have our data $X$, $D$, $y$, of controls, treatments and outcomes. In some datasets, we also have \"groups\", also known as \"clusters\". These are group ids, that define a group of samples that are believed to be correlated through unobesrved factors. For instance, in randomized experiments when a whole class is being treated and we have data at the student level, the students in a class constitute a cluster, as their outcome variables are most probably correlated. In such settings, it is helpful to account for the cluster correlations when calculating confidence intervals and when performing sample splitting for either cross-validation or for nuisance estimation.\n",
    "\n",
    "We will be assuming throughout that conditional ignorability is satisfied if we control for all the variables $X$, i.e. the potential outcomes $Y(1), Y(0)$ satisfy\n",
    "\\begin{align}\n",
    "Y(1), Y(0) ~\\perp\\hspace{-1em}\\perp~D \\mid X\n",
    "\\end{align}\n",
    "Equivalently, we assume that the DAG the corresponds to our setting satisfies that $X$ is a valid adjustment set between $D$ and $Y$, i.e. it blocks all backdoor paths in the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>inc</th>\n",
       "      <th>fsize</th>\n",
       "      <th>educ</th>\n",
       "      <th>db</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>twoearn</th>\n",
       "      <th>pira</th>\n",
       "      <th>nohs</th>\n",
       "      <th>hs</th>\n",
       "      <th>smcol</th>\n",
       "      <th>col</th>\n",
       "      <th>hown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "      <td>9716.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.034891</td>\n",
       "      <td>36462.224269</td>\n",
       "      <td>2.868361</td>\n",
       "      <td>13.192672</td>\n",
       "      <td>0.271202</td>\n",
       "      <td>0.603746</td>\n",
       "      <td>0.205949</td>\n",
       "      <td>0.380198</td>\n",
       "      <td>0.238678</td>\n",
       "      <td>0.127419</td>\n",
       "      <td>0.379477</td>\n",
       "      <td>0.247118</td>\n",
       "      <td>0.245986</td>\n",
       "      <td>0.635241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.337014</td>\n",
       "      <td>22193.821846</td>\n",
       "      <td>1.541154</td>\n",
       "      <td>2.787505</td>\n",
       "      <td>0.444603</td>\n",
       "      <td>0.489143</td>\n",
       "      <td>0.404414</td>\n",
       "      <td>0.485460</td>\n",
       "      <td>0.426298</td>\n",
       "      <td>0.333459</td>\n",
       "      <td>0.485282</td>\n",
       "      <td>0.431358</td>\n",
       "      <td>0.430692</td>\n",
       "      <td>0.481387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>19648.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>31473.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>48071.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>118599.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age            inc        fsize         educ           db  \\\n",
       "count  9716.000000    9716.000000  9716.000000  9716.000000  9716.000000   \n",
       "mean     41.034891   36462.224269     2.868361    13.192672     0.271202   \n",
       "std      10.337014   22193.821846     1.541154     2.787505     0.444603   \n",
       "min      25.000000    4080.000000     1.000000     1.000000     0.000000   \n",
       "25%      32.000000   19648.500000     2.000000    12.000000     0.000000   \n",
       "50%      40.000000   31473.000000     3.000000    12.000000     0.000000   \n",
       "75%      48.000000   48071.250000     4.000000    15.000000     1.000000   \n",
       "max      64.000000  118599.000000    13.000000    18.000000     1.000000   \n",
       "\n",
       "              marr         male      twoearn         pira         nohs  \\\n",
       "count  9716.000000  9716.000000  9716.000000  9716.000000  9716.000000   \n",
       "mean      0.603746     0.205949     0.380198     0.238678     0.127419   \n",
       "std       0.489143     0.404414     0.485460     0.426298     0.333459   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     1.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "                hs        smcol          col         hown  \n",
       "count  9716.000000  9716.000000  9716.000000  9716.000000  \n",
       "mean      0.379477     0.247118     0.245986     0.635241  \n",
       "std       0.485282     0.431358     0.430692     0.481387  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     1.000000  \n",
       "75%       1.000000     0.000000     0.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(D)\n",
    "plt.title('Treatment')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y)\n",
    "plt.title('Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train and Validation and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data will be used to fit the various CATE models. The validation data will be used for scoring and selection of the best CATE model or best ensemble of CATE models. The test data will be used for testing and evaluation of the performance of the best chosen model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "if groups is None:\n",
    "    X, Xval, D, Dval, y, yval = train_test_split(X, D, y, train_size=.6, shuffle=True,\n",
    "                                                 random_state=random_seed)\n",
    "    Xval, Xtest, Dval, Dtest, yval, ytest = train_test_split(Xval, Dval, yval, train_size=.5, shuffle=True,\n",
    "                                                             random_state=random_seed)\n",
    "    groupsval, groupstest = None, None\n",
    "else:\n",
    "    train, val = next(GroupShuffleSplit(n_splits=2, train_size=.6,\n",
    "                                        random_state=random_seed).split(X, y, groups=groups))\n",
    "    X, Xval, D, Dval, y, yval = X.iloc[train], X.iloc[val], D[train], D[val], y[train], y[val]\n",
    "    groups, groupsval = groups[train], groups[val]\n",
    "\n",
    "    val, test = next(GroupShuffleSplit(n_splits=2, train_size=.5,\n",
    "                                       random_state=random_seed).split(Xval, yval, groups=groupsval))\n",
    "    Xval, Xtest, Dval, Dtest, yval, ytest = Xval.iloc[val], Xval.iloc[test], Dval[val], Dval[test], yval[val], yval[test]\n",
    "    groupsval, groupstest = groupsval[val], groupsval[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuisance Model Selection\n",
    "\n",
    "Using the training data we will select the best model for each of the nuisance models that arise in meta learner CATE approaches. They five models correspond to the following five predictive problems:\n",
    "\\begin{align}\n",
    "\\text{model_reg} ~\\rightarrow~& g(d, x) := E[Y\\mid D=d, X=x]\\\\\n",
    "\\text{model_y} ~\\rightarrow~& q(x) := E[Y\\mid X=x]\\\\\n",
    "\\text{model_t} ~\\rightarrow~& p(x) := E[D\\mid X=x] = \\Pr(D=1\\mid X=x)\\\\\n",
    "\\text{model_reg_zero} ~\\rightarrow~& g_0(x) := E[Y\\mid D=0, X=x]\\\\\n",
    "\\text{model_reg_one} ~\\rightarrow~& g_1(x) := E[Y\\mid D=1, X=x]\\\\\n",
    "\\end{align}\n",
    "We will select the best hyperparameters/model type for each predictive problem using cross-validation, where the splits are also stratified by the treatment (so that we have balanced split of the treatment groups across folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if groups is None:\n",
    "#     split_type = 'auto'\n",
    "# else:\n",
    "#     split_type = GroupKFold(n_splits=n_splits)\n",
    "    \n",
    "# # These function calls perform auto-ml hyperparameter tuning and return a \"model class generator\"\n",
    "# # i.e. a function that whenever called returns an instance of an un-fitted model with the best hyper-parameters\n",
    "# if binary_y:\n",
    "#     model_reg = auto_clf(np.column_stack((D, X)), y, groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                          verbose=verbose, time_budget=time_budget)\n",
    "#     model_y = auto_clf(X, y, n_splits=n_splits, split_type=split_type, \n",
    "#                        verbose=verbose, time_budget=time_budget)\n",
    "#     model_reg_zero = auto_clf(X[D==0], y[D==0], groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                               verbose=verbose, time_budget=time_budget)\n",
    "#     model_reg_one = auto_clf(X[D==1], y[D==1], groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                              verbose=verbose, time_budget=time_budget)\n",
    "# else:\n",
    "#     model_reg = auto_reg(np.column_stack((D, X)), y, groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                          verbose=verbose, time_budget=time_budget)\n",
    "#     model_y = auto_reg(X, y, groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                        verbose=verbose, time_budget=time_budget)\n",
    "#     model_reg_zero = auto_reg(X[D==0], y[D==0], groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                               verbose=verbose, time_budget=time_budget)\n",
    "#     model_reg_one = auto_reg(X[D==1], y[D==1], groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                              verbose=verbose, time_budget=time_budget)\n",
    "# model_t = auto_clf(X, D, groups=groups, n_splits=n_splits, split_type=split_type, \n",
    "#                    verbose=verbose, time_budget=time_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myxgb import xgb_reg, xgb_clf, xgb_wreg\n",
    "\n",
    "auto_reg = lambda: xgb_reg(random_seed)\n",
    "auto_clf = lambda: xgb_clf(random_seed)\n",
    "def auto_weighted_reg(*args, **kwargs):\n",
    "    return lambda: xgb_wreg(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = auto_reg\n",
    "model_y = auto_reg\n",
    "model_reg_zero = auto_reg\n",
    "model_reg_one = auto_reg\n",
    "model_t = auto_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MyXGBRegressor</label><div class=\"sk-toggleable__content\"><pre>MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MyXGBRegressor</label><div class=\"sk-toggleable__content\"><pre>MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MyXGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "               enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "               gamma=None, grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "               min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "               num_parallel_tree=None, random_state=12, ...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MyXGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "                min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "                num_parallel_tree=None, random_state=12, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MyXGBClassifier</label><div class=\"sk-toggleable__content\"><pre>MyXGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "                min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "                num_parallel_tree=None, random_state=12, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MyXGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, device=None, early_stopping_rounds=5,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=2, max_leaves=None,\n",
       "                min_child_weight=20, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=500, n_jobs=None,\n",
       "                num_parallel_tree=None, random_state=12, ...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save or load these models from a previous run un-comment the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump([model_reg(), model_y(), model_t(), model_reg_zero(), model_reg_one()], 'nuisance.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mreg, my, mt, mreg_zero, mreg_one = joblib.load('nuisance.jbl')\n",
    "# model_reg = lambda: clone(mreg)\n",
    "# model_y = lambda: clone(my)\n",
    "# model_t = lambda: clone(mt)\n",
    "# model_reg_zero = lambda: clone(mreg_zero)\n",
    "# model_reg_one = lambda: clone(mreg_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now also evaluate the performance of the selected models in terms of R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if groups is None:\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "else:\n",
    "    cv = GroupKFold(n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_reg = np.mean(cross_val_score(model_reg(), X, y, groups=groups, cv=cv, scoring='r2'))\n",
    "print(f'model_reg: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_y(), X, y, groups=groups, cv=cv, scoring='r2'))\n",
    "print(f'model_y: {score_reg:.3f}')\n",
    "score_reg = np.mean(cross_val_score(model_t(), X, D, groups=groups, cv=cv, scoring='r2'))\n",
    "print(f'model_t: {score_reg:.3f}')\n",
    "if groups is None:\n",
    "    score_reg = np.mean(cross_val_score(model_reg_zero(), X[D==0], y[D==0], groups=None, cv=cv, scoring='r2'))\n",
    "    print(f'model_reg_zero: {score_reg:.3f}')\n",
    "    score_reg = np.mean(cross_val_score(model_reg_one(), X[D==1], y[D==1], groups=None, cv=cv, scoring='r2'))\n",
    "    print(f'model_reg_one: {score_reg:.3f}')\n",
    "else:\n",
    "    score_reg = np.mean(cross_val_score(model_reg_zero(), X[D==0], y[D==0], groups=groups[D==0], cv=cv, scoring='r2'))\n",
    "    print(f'model_reg_zero: {score_reg:.3f}')\n",
    "    score_reg = np.mean(cross_val_score(model_reg_one(), X[D==1], y[D==1], groups=groups[D==1], cv=cv, scoring='r2'))\n",
    "    print(f'model_reg_one: {score_reg:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuisance Cross-Fitted Estimation and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After selecting the hyper-parameters for each of the nuisance models we perform cross-fitting to get out-of-fold predictions from each of these nuisance models. At the end of this process, we will have for each sample $i$, the following out-of-fold nuisance values:\n",
    "\\begin{align}\n",
    "\\text{reg_preds} \\rightarrow~& \\hat{g}(D_i, X_i) &\n",
    "\\text{reg_one_preds} \\rightarrow~& \\hat{g}(1, X_i) &\n",
    "\\text{reg_zero_preds} \\rightarrow~& \\hat{g}(0, X_i)\\\\\n",
    "\\text{reg_preds_t} \\rightarrow~& \\hat{g}_0(X_i) (1 - D_i) + \\hat{g}_1(X_i) D_i &\n",
    "\\text{reg_one_preds_t} \\rightarrow~& \\hat{g}_1(X_i) &\n",
    "\\text{reg_zero_preds_t} \\rightarrow~& \\hat{g}_0(X_i)\\\\\n",
    "\\text{res_preds} \\rightarrow~& \\hat{q}(X_i) &\n",
    "\\text{prop_preds} \\rightarrow~& \\hat{p}(X_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfit:\n",
    "    if groups is None:\n",
    "        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        splits = list(cv.split(X, D))\n",
    "    else:\n",
    "        cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "        splits = list(cv.split(X, D, groups=groups))\n",
    "else:\n",
    "    splits = [(np.arange(X.shape[0]), np.arange(X.shape[0]))]\n",
    "\n",
    "n = X.shape[0]\n",
    "reg_preds = np.zeros(n)\n",
    "reg_zero_preds = np.zeros(n)\n",
    "reg_one_preds = np.zeros(n)\n",
    "reg_preds_t = np.zeros(n)\n",
    "reg_zero_preds_t = np.zeros(n)\n",
    "reg_one_preds_t = np.zeros(n)\n",
    "\n",
    "DX = np.column_stack((D, X))\n",
    "for train, test in splits:\n",
    "    reg = model_reg().fit(DX[train], y[train])\n",
    "    reg_preds[test] = reg.predict(DX[test])\n",
    "    reg_one_preds[test] = reg.predict(np.column_stack([np.ones(len(test)), X.iloc[test]]))\n",
    "    reg_zero_preds[test] = reg.predict(np.column_stack([np.zeros(len(test)), X.iloc[test]]))\n",
    "\n",
    "    reg_zero = model_reg_zero().fit(X.iloc[train][D[train]==0], y[train][D[train]==0])\n",
    "    reg_one = model_reg_one().fit(X.iloc[train][D[train]==1], y[train][D[train]==1])\n",
    "    reg_zero_preds_t[test] = reg_zero.predict(X.iloc[test])\n",
    "    reg_one_preds_t[test] = reg_one.predict(X.iloc[test])\n",
    "    reg_preds_t[test] = reg_zero_preds_t[test] * (1 - D[test]) + reg_one_preds_t[test] * D[test]\n",
    "\n",
    "res_preds = cross_val_predict(model_y(), X, y, cv=splits)\n",
    "prop_preds = cross_val_predict(model_t(), X, D, cv=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATE Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the doubly robust method. In particular, we construct the doubly robust variables\n",
    "\\begin{align}\n",
    "Y_i^{DR}(\\hat{g},\\hat{p}) := \\hat{g}_1(X_i) - \\hat{g}_0(X_i) + (Y_i - \\hat{g}_{D_i}(X_i))\\frac{D_i - \\hat{p}(X_i)}{\\hat{p}(X_i) (1-\\hat{p}(X_i))}\n",
    "\\end{align}\n",
    "and then we estimate:\n",
    "\\begin{align}\n",
    "ATE = E_n\\left[Y^{DR}(\\hat{g},\\hat{p})\\right]\n",
    "\\end{align}\n",
    "This should be more efficient in the worst-case and should be returning a consistent estimate of the ATE even beyond RCTs and will also correctly account for any imbalances or violations of the randomization assumption in an RCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_preds = reg_one_preds_t - reg_zero_preds_t\n",
    "dr_preds += (y - reg_preds_t) * (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), cov_clip, np.inf)\n",
    "\n",
    "if groups is None:\n",
    "    display(OLS(dr_preds, np.ones((len(dr_preds), 1))).fit(cov_type='HC1').summary())\n",
    "else:\n",
    "    display(OLS(dr_preds, np.ones((len(dr_preds), 1))).fit(cov_type='cluster', cov_kwds={'groups': groups}).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Linear CATE Predictor and Simultaneous (Joint) Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the doubly robust variables as pseudo-outcomes in an OLS regression, so as to estimate the best linear approximation of the true CATE. In an RCT, these should be similar to the coefficients recovered in a plain interactive OLS regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_preds = reg_one_preds_t - reg_zero_preds_t\n",
    "dr_preds += (y - reg_preds_t) * (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), cov_clip, np.inf)\n",
    "\n",
    "dfX = X.copy()\n",
    "dfX = dfX - dfX.mean(axis=0)\n",
    "dfX['const'] = 1\n",
    "if groups is None:\n",
    "    lr = OLS(dr_preds, dfX).fit(cov_type='HC1')\n",
    "    cov = lr.get_robustcov_results(cov_type='HC1')\n",
    "else:\n",
    "    lr = OLS(dr_preds, dfX).fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "    cov = lr.get_robustcov_results(cov_type='cluster', groups=groups)\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform joint inference on all these parameters controlling the joint probability of failure of the confidence intervals by 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = cov.cov_params()\n",
    "S = np.diag(np.diagonal(V)**(-1/2))\n",
    "epsilon = np.random.multivariate_normal(np.zeros(V.shape[0]), S @ V @ S, size=(1000))\n",
    "critical = np.percentile(np.max(np.abs(epsilon), axis=1), 95)\n",
    "stderr = np.diagonal(V)**(1/2)\n",
    "lb = cov.params - critical * stderr\n",
    "ub = cov.params + critical * stderr\n",
    "jointsummary = pd.DataFrame({'coef': cov.params,\n",
    "                             'std err': stderr,\n",
    "                             'lb': lb,\n",
    "                             'ub': ub,\n",
    "                             'statsig': ['' if ((l <= 0) & (0 <= u)) else '**' for (l, u) in zip(lb, ub)]},\n",
    "                            index=dfX.columns)\n",
    "display(jointsummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce confidence intervals for the predictions of the CATE at particular points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.unique(np.percentile(dfX[xfeat], np.arange(0, 110, 20)))\n",
    "\n",
    "Zpd = pd.DataFrame(np.tile(np.median(dfX, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=dfX.columns)\n",
    "Zpd[xfeat] = grid\n",
    "\n",
    "pred_df = lr.get_prediction(Zpd).summary_frame()\n",
    "preds, lb, ub = pred_df['mean'].values, pred_df['mean_ci_lower'].values, pred_df['mean_ci_upper'].values\n",
    "preds = preds.flatten()\n",
    "lb = lb.flatten()\n",
    "ub = ub.flatten()\n",
    "plt.errorbar(Zpd[xfeat], preds, yerr=(preds-lb, ub-preds))\n",
    "plt.xlabel(xfeat)\n",
    "plt.ylabel('Predicted CATE (at median value of other features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even simultaneous inference on all these predictions that controls the joint failure probability of these confidence intervals to be at most 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsV = Zpd.values @ V @ Zpd.values.T\n",
    "predsS = np.diag(np.diagonal(predsV)**(-1/2))\n",
    "epsilon = np.random.multivariate_normal(np.zeros(predsV.shape[0]), predsS @ predsV @ predsS, size=(1000))\n",
    "critical = np.percentile(np.max(np.abs(epsilon), axis=1), 95)\n",
    "stderr = np.diagonal(predsV)**(1/2)\n",
    "lb = preds - critical * stderr\n",
    "ub = preds + critical * stderr\n",
    "\n",
    "plt.errorbar(Zpd[xfeat], preds, yerr=(preds-lb, ub-preds))\n",
    "plt.xlabel(xfeat)\n",
    "plt.ylabel('Predicted CATE (at median value of other features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "df = X.copy()\n",
    "df['dr'] = dr_preds\n",
    "if groups is None:\n",
    "    lr = ols('dr ~ ' + blp_formula, df).fit(cov_type='HC1')\n",
    "else:\n",
    "    lr = ols('dr ~ ' + blp_formula, df).fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.unique(np.percentile(X[xfeat], np.arange(0, 102, 2)))\n",
    "Xpd = pd.DataFrame(np.tile(np.median(X, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=X.columns)\n",
    "Xpd[xfeat] = grid\n",
    "pred_df = lr.get_prediction(Xpd).summary_frame(alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xpd[xfeat], pred_df['mean'])\n",
    "plt.fill_between(Xpd[xfeat], pred_df['mean_ci_lower'], pred_df['mean_ci_upper'], alpha=.4)\n",
    "plt.xlabel(xfeat + ' (other features fixed at median value)')\n",
    "plt.title('Predicted CATE BLP: cate ~' + blp_formula)\n",
    "plt.ylabel('CATE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "df = X.copy()\n",
    "df['dr'] = dr_preds\n",
    "if groups is None:\n",
    "    lr = ols('dr ~ ' + blp_formula2, df).fit(cov_type='HC1')\n",
    "else:\n",
    "    lr = ols('dr ~ ' + blp_formula2, df).fit(cov_type='cluster', cov_kwds={'groups': groups})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.unique(np.percentile(X[xfeat], np.arange(0, 102, 2)))\n",
    "Xpd = pd.DataFrame(np.tile(np.median(X, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=X.columns)\n",
    "Xpd[xfeat] = grid\n",
    "pred_df2 = lr.get_prediction(Xpd).summary_frame(alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Xpd[xfeat], pred_df2['mean'])\n",
    "plt.fill_between(Xpd[xfeat], pred_df2['mean_ci_lower'], pred_df2['mean_ci_upper'], alpha=.4)\n",
    "plt.xlabel(xfeat + ' (other features fixed at median value)')\n",
    "plt.ylabel('CATE')\n",
    "plt.title('cate ~' + blp_formula2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE Model Estimation with Meta-Learners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify which indices of the X variables we want to use for heterogeneity. Let's denote these subset of variables with $Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hetero_feats == 'all':\n",
    "    hetero_feats = X.columns\n",
    "Z, Zval, Ztest = X[hetero_feats], Xval[hetero_feats], Xtest[hetero_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify a generic automl approach for training the final CATE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final_fn = lambda Z, y: auto_reg(Z, y, groups=groups,\n",
    "                                       n_splits=n_splits, split_type=split_type, \n",
    "                                       verbose=verbose, time_budget=time_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final_fn = lambda Z, y: auto_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Learner (S-Learner)\n",
    "\n",
    "The S-Learner simply trains a model to predict:\n",
    "\\begin{align}\n",
    "\\hat{g}(1, X_i) - \\hat{g}(0, X_i) \\sim Z_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# slearner\n",
    "slearner_best = model_final_fn(Z, reg_one_preds - reg_zero_preds)\n",
    "slearner = slearner_best().fit(Z, reg_one_preds - reg_zero_preds)\n",
    "slearner_cates = slearner.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Learner (T-Learner)\n",
    "\n",
    "The T-Learner simply trains a model to predict:\n",
    "\\begin{align}\n",
    "\\hat{g}_1(X_i) - \\hat{g}_0(X_i) \\sim Z_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tlearner\n",
    "tlearner_best = model_final_fn(Z, reg_one_preds_t - reg_zero_preds_t)\n",
    "tlearner = tlearner_best().fit(Z, reg_one_preds_t - reg_zero_preds_t)\n",
    "tlearner_cates = tlearner.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Learner (X-Learner)\n",
    "\n",
    "The X-Learner simply trains a two treatment effect models using all the variables $X$:\n",
    "\\begin{align}\n",
    "\\hat{\\tau}_1: Y_i - \\hat{g}_0(X_i) \\sim X_i \\\\\n",
    "\\hat{\\tau}_0: \\hat{g}_1(X_i) - Y_i \\sim X_i\n",
    "\\end{align}\n",
    "And then for each target sample $X_i$ we select a mixture based on the propensity. \n",
    "If $D=1$ is more probable, then we use more heavily $\\hat{\\tau}_0$, since that uses the model $\\hat{g}_1$ which was trained on more data similar to $X_i$ (as indicated by the propensity). Similarly, if $D=0$ is more probably we put more weight on $\\hat{\\tau}_1$. \n",
    "\\begin{align}\n",
    "\\hat{\\tau}(X_i) = \\hat{p}(X_i) \\hat{\\tau}_0(X_i) + (1 - \\hat{p}(X_i)) \\hat{\\tau}_1(X_i)\n",
    "\\tag{xtarget}\n",
    "\\end{align}\n",
    "Finally, to learn a CATE model that only depends on $Z$, we predict the CATEs $\\hat{\\tau}$ from $Z$:\n",
    "\\begin{align}\n",
    "\\hat{\\tau}(X_i) \\sim Z_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlearner\n",
    "tau1_preds = y[D==1] - reg_zero_preds_t[D==1]\n",
    "tau0_preds = reg_one_preds_t[D==0] - y[D==0]\n",
    "tau1 = model_final_fn(X[D==1], tau1_preds)().fit(X[D==1], tau1_preds)\n",
    "tau0 = model_final_fn(X[D==0], tau0_preds)().fit(X[D==0], tau0_preds)\n",
    "xtarget = prop_preds * tau0.predict(X) + (1 - prop_preds) * tau1.predict(X)\n",
    "xlearner = model_final_fn(Z, xtarget)().fit(Z, xtarget)\n",
    "xlearner_cates = xlearner.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubly Robust Learner (DR-Learner)\n",
    "\n",
    "The DR-Learner (in particular, the variant based on the T-Learner) adds a de-biasing correction to the T-Learner using the propensity. In particular, we construct the doubly robust variables\n",
    "\\begin{align}\n",
    "Y_i^{DR}(\\hat{g},\\hat{p}) := \\hat{g}_1(X_i) - \\hat{g}_0(X_i) + (Y_i - \\hat{g}_{D_i}(X_i))\\frac{D_i - \\hat{p}(X_i)}{\\hat{p}(X_i) (1-\\hat{p}(X_i))}\n",
    "\\end{align}\n",
    "and then we train a CATE model by predicting these variables from $Z_i$:\n",
    "\\begin{align}\n",
    "Y_i^{DR}(\\hat{g},\\hat{p}) \\sim Z_i\n",
    "\\end{align}\n",
    "For stability we clip the co-variance that appears in the denominator at some value $c$ bounded away from zero, i.e.\n",
    "\\begin{align}\n",
    "Y_i^{DR}(\\hat{g},\\hat{p}) := \\hat{g}_1(X_i) - \\hat{g}_0(X_i) + (Y_i - \\hat{g}_{D_i}(X_i))\\frac{D_i - \\hat{p}(X_i)}{\\min\\{c, \\hat{p}(X_i) (1-\\hat{p}(X_i))\\}} \n",
    "\\tag{dr_preds}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drlearner\n",
    "dr_preds = reg_one_preds_t - reg_zero_preds_t\n",
    "dr_preds += (y - reg_preds_t) * (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), cov_clip, np.inf)\n",
    "drlearner_best = model_final_fn(Z, dr_preds)\n",
    "drlearner = drlearner_best().fit(Z, dr_preds)\n",
    "drlearner_cates = drlearner.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Learner (R-Learner)\n",
    "\n",
    "The R-Learner uses the fact that if the true CATE model \n",
    "$$\\theta(X)=E[Y(1)-Y(0)\\mid X]$$\n",
    "only depends on the subset of variables $Z$, then we can write:\n",
    "\\begin{align}\n",
    "Y =~& \\tau(Z) D + b(X) + \\epsilon, & E[\\epsilon\\mid D, X]=~& 0\n",
    "\\end{align}\n",
    "This then implies that we can also write:\n",
    "\\begin{align}\n",
    "Y - E[Y\\mid X] =~& \\tau(Z) (D - E[D|X]) + \\epsilon, & E[\\epsilon\\mid D, X]=~& 0\n",
    "\\end{align}\n",
    "Hence, if we define the residual outcome and the residual treatment, \n",
    "\\begin{align}\n",
    "\\tilde{Y} :=~& Y - E[Y\\mid X] \\tag{yres}\\\\\n",
    "\\tilde{D} :=~& D - E[D\\mid X] \\tag{Dres}\n",
    "\\end{align}\n",
    "then we have:\n",
    "\\begin{align}\n",
    "\\tilde{Y} =~& \\tau(Z) \\tilde{D} + \\epsilon, & E[\\epsilon\\mid D, X]=~& 0\n",
    "\\end{align}\n",
    "Thus we can estimate the CATE by predicting $\\tilde{Y}$ from $\\tilde{D}, Z$ with a separable model of the form $\\tau(Z) \\tilde{D}$. This also implies that $\\tau$ is the minimizer of the square loss:\n",
    "\\begin{align}\n",
    "E\\left[ \\left(\\tilde{Y} - \\tau(Z) \\tilde{D}\\right)^2 \\right]\n",
    "\\end{align}\n",
    "This loss can also be re-written as a sample-weighted regression loss:\n",
    "\\begin{align}\n",
    "E\\left[ \\tilde{D}^2 \\left(\\tilde{Y}/\\tilde{D} - \\tau(Z) \\right)^2 \\right]\n",
    "\\end{align}\n",
    "We are predicting $\\tilde{Y}/\\tilde{D}$ form $Z$, using sample weights $\\tilde{D}^2$.\n",
    "\n",
    "If the true CATE $\\theta(X)$ did not only depend on $Z$, but on the bigger set of covariates $X$, then this method estimates the best projection of the CATE $\\theta(X)$ on the space of functions of $Z$, albeit, in a weighted manner, weighted by the variance of the treatment. More formally, the method estimates the minimizer of the following loss:\n",
    "\\begin{align}\n",
    "\\min_{\\tau} E\\left[\\left(\\theta(X) - \\tau(Z)\\right)^2 \\, \\text{Var}(D\\mid X)\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rlearner\n",
    "yres = y - res_preds\n",
    "Dres = D - prop_preds\n",
    "Dres = np.clip(Dres, 1e-6, np.inf) * (Dres >= 0) + np.clip(Dres, -np.inf, -1e-6) * (Dres < 0)\n",
    "\n",
    "rlearner_fn = auto_weighted_reg(Z, yres / Dres, sample_weight=Dres**2, groups=groups,\n",
    "                                n_splits=n_splits, verbose=verbose, time_budget=time_budget)\n",
    "rlearner = rlearner_fn().fit(Z, yres / Dres, sample_weight=Dres**2)\n",
    "\n",
    "rlearner_cates = rlearner.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Effect DR-Learner\n",
    "\n",
    "We also add a heavily regularized CATE model that predicts the ATE using the doubly robust pseudo outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drlearner_const = make_pipeline(PolynomialFeatures(degree=0, include_bias=True), \n",
    "                                LinearRegression(fit_intercept=False)).fit(Z, dr_preds)\n",
    "drlearner_const_cates = drlearner_const.predict(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Score Estimation and Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to select among all these different meta learners. For this reason we will use scoring functions that can evaluate the performance of an arbitrary CATE function and is not tailored to any particular methodology. This way we can evaluate all methods using the same score on the validation set and select the best among the methods, or ensemble the methods using this scoring metric. We will describe two such meta scores, the `R-score` and the `DR-score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Doubly Robust Score (DR-Score)\n",
    "\n",
    "The doubly robust score calculates in a cross-fitting manner, using only the validation set, the doubly robust proxy variables $Y_i^{DR}(g,p)$, where $g,p$ are fitted in the validation set in the cross-fitting manner (or if the validation set is small, we can use the training set to estimate the nuisance models). Then for any candidate CATE model $\\hat{\\tau}$, the DR-score is simply the R^2 in the regression predicting $Y_i^{DR}(g,p)$ using $Z$, i.e.\n",
    "\\begin{align}\n",
    "\\text{DRscore}(\\hat{\\tau}) := 1 - \\frac{E_n\\left[\\left(Y^{DR}(g,p) - \\hat{\\tau}(Z)\\right)^2\\right]}{\\text{Var}\\left(Y^{DR}(g,p)\\right)}\n",
    "\\end{align}\n",
    "This can also be interpreted as normalizing by the best constant CATE model, since in this case the best constant CATE model is simply $\\tau^*=E_n[Y^{DR}(g,p)]$ and therefore:\n",
    "\\begin{align}\n",
    "\\text{DRscore}(\\hat{\\tau}) = 1 - \\frac{E_n\\left[\\left(Y^{DR}(g,p) - \\hat{\\tau}(Z)\\right)^2\\right]}{E_n\\left[\\left(Y^{DR}(g,p) - \\tau^*\\right)^2\\right]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_dr_outcomes(Xtrain, Dtrain, ytrain, groupstrain, X, D, y, groups):\n",
    "#     if groups is None:\n",
    "#         cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "#         splits = list(cv.split(X, D))\n",
    "#     else:\n",
    "#         cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
    "#         splits = list(cv.split(X, D, groups=groups))\n",
    "\n",
    "#     n = X.shape[0]\n",
    "#     reg_preds_t = np.zeros(n)\n",
    "#     reg_zero_preds_t = np.zeros(n)\n",
    "#     reg_one_preds_t = np.zeros(n)\n",
    "\n",
    "#     for train, test in splits:\n",
    "#         reg_zero = model_reg_zero().fit(X.iloc[train][D[train]==0], y[train][D[train]==0])\n",
    "#         reg_one = model_reg_one().fit(X.iloc[train][D[train]==1], y[train][D[train]==1])\n",
    "#         reg_zero_preds_t[test] = reg_zero.predict(X.iloc[test])\n",
    "#         reg_one_preds_t[test] = reg_one.predict(X.iloc[test])\n",
    "#         reg_preds_t[test] = reg_zero_preds_t[test] * (1 - D[test]) + reg_one_preds_t[test] * D[test]\n",
    "\n",
    "#     prop_preds = cross_val_predict(model_t(), X, D, cv=splits)\n",
    "\n",
    "#     dr = reg_one_preds_t - reg_zero_preds_t\n",
    "#     reisz = (D - prop_preds) / np.clip(prop_preds * (1 - prop_preds), cov_clip, np.inf)\n",
    "#     dr += (y - reg_preds_t) * reisz\n",
    "\n",
    "#     return dr\n",
    "\n",
    "def calculate_dr_outcomes(Xtrain, Dtrain, ytrain, groupstrain, Xval, Dval, yval, groupsval):\n",
    "\n",
    "    reg_zero = model_reg_zero().fit(Xtrain[Dtrain==0], ytrain[Dtrain==0])\n",
    "    reg_one = model_reg_one().fit(Xtrain[Dtrain==1], ytrain[Dtrain==1])\n",
    "    reg_zero_preds_t = reg_zero.predict(Xval)\n",
    "    reg_one_preds_t = reg_one.predict(Xval)\n",
    "    reg_preds_t = reg_zero_preds_t * (1 - Dval) + reg_one_preds_t * Dval\n",
    "    prop_preds = model_t().fit(Xtrain, Dtrain).predict(Xval)\n",
    "\n",
    "    dr = reg_one_preds_t - reg_zero_preds_t\n",
    "    reisz = (Dval - prop_preds) / np.clip(prop_preds * (1 - prop_preds), .01, np.inf)\n",
    "    dr += (yval - reg_preds_t) * reisz\n",
    "\n",
    "    return dr\n",
    "\n",
    "dr_val = calculate_dr_outcomes(X, D, y, groups, Xval, Dval, yval, groupsval)\n",
    "\n",
    "overall_ate_val_dr = np.mean(dr_preds)\n",
    "\n",
    "def drscore(cate_preds):\n",
    "    drscore_t = np.mean((dr_val - cate_preds)**2)\n",
    "    drscore_b = np.mean((dr_val - overall_ate_val_dr)**2)\n",
    "    return drscore_b - drscore_t \n",
    "\n",
    "def drscore_delta(cate_preds):\n",
    "    drscore_t = np.mean((dr_val - cate_preds)**2)\n",
    "    drscore_b = np.mean((dr_val - overall_ate_val_dr)**2)\n",
    "    stderr = np.std((dr_val - overall_ate_val_dr)**2 - (dr_val - cate_preds)**2) / np.sqrt(cate_preds.shape[0])\n",
    "    return drscore_b - drscore_t, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score CATE Models\n",
    "\n",
    "We first chose one of the scorers and we score all the CATE models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = drscore\n",
    "score_name = 'DRscore'\n",
    "names = ['slearner', 'tlearner', 'xlearner', 'drlearner', 'rlearner', 'drlearner_const']\n",
    "models = [slearner, tlearner, xlearner, drlearner, rlearner, drlearner_const]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [scorer(model.predict(Zval)) for model in models]\n",
    "print([f'{name}: {score:.4f}' for name, score in zip(names, scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in zip(names, models):\n",
    "    score = drscore_delta(model.predict(Zval))\n",
    "    print(f'{name}: {score[0]:.4f} ({score[1]:.4f}) '\n",
    "          f'[{score[0] - 1.96 * score[1]:.4f}, {score[0] + 1.96 * score[1]:.4f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting CATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = ['Single Learner', 'Two Learner', 'Cross Learner', \n",
    "          'Doubly Robust Learner', 'Residual Learner', 'Doubly Robust ATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    grid = np.unique(np.percentile(X[xfeat], np.arange(0, 102, 2)))\n",
    "    Xpd = pd.DataFrame(np.tile(np.median(X, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                        columns=X.columns)\n",
    "    Xpd[xfeat] = grid\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for it, (fname, name, model, score) in enumerate(zip(fnames, names, models, scores)):\n",
    "        plt.subplot(3, len(models)//3, it + 1)\n",
    "        preds = model.predict(Xpd[hetero_feats])\n",
    "        plt.plot(Xpd[xfeat], preds, linewidth=5, color='green', label=name)\n",
    "        plt.plot(Xpd[xfeat], pred_df['mean'], '--', color='orange', label='$Y(\\hat{\\eta})\\sim log(inc)$')\n",
    "        plt.fill_between(Xpd[xfeat], pred_df['mean_ci_lower'], pred_df['mean_ci_upper'], color='orange', alpha=.1)\n",
    "        plt.plot(Xpd[xfeat], pred_df2['mean'], '--', color='blue', label='$Y(\\hat{\\eta})\\sim poly(log(inc), 4)$')\n",
    "        plt.fill_between(Xpd[xfeat], pred_df2['mean_ci_lower'], pred_df2['mean_ci_upper'], color='blue', alpha=.1)\n",
    "        if semi_synth:\n",
    "            plt.plot(Xpd[xfeat], true_cate(Xpd), label='True')\n",
    "            plt.title(f'{score_name}={score:.4f}, True RMSE={rmse(true_cate(X), model.predict(Z)):.5f}')\n",
    "        else:\n",
    "            plt.title(f'{fname}. ATE={np.mean(model.predict(Zval)):.0f}', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 50000)\n",
    "        plt.xlabel(xfeat, fontsize=14)\n",
    "        plt.ylabel('CATE', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{data}-metalearners.png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Model Selection and Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use these scores to create an ensemble CATE model of the different methods based on the score performance. We want to create a new CATE model $\\tau_E$ that is a weighted linear combination of all the cate models, i.e.\n",
    "\\begin{align}\n",
    "\\tau_E(Z) = \\bar{\\tau} + \\sum_{m=1}^M w_m \\tau_m(Z)\n",
    "\\end{align}\n",
    "One way to achieve that is to choose the model with the best score. However, the discontinuity in this selection process can be statistically suboptimal. One way to achieve this in a smoother manner is to perform Stacking to construct the ensemble weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Ensemble(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, names, models, weights, intercept=0):\n",
    "        self.names = names\n",
    "        self.models = models\n",
    "        self.weights = weights\n",
    "        self.intercept = intercept\n",
    "    \n",
    "    def predict(self, X):\n",
    "        wcate = np.sum(self.weights.reshape((-1, 1)) * np.array([m.predict(X) for m in self.models]), axis=0)\n",
    "        return self.intercept + wcate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform Stacking by fitting a (potentially $\\ell_1$-penalized) linear model to minimize the corresponding loss. For the case of the DR score this boils down to a penalized linear regression, predicting $Y^{DR}(g,p)$ on the validation set, using $\\tau_1(Z),\\ldots, \\tau_M(Z)$ as technical regressors:\n",
    "\\begin{align}\n",
    "\\min_{w} E_n\\left[ \\left(Y^{DR}(g,p) - \\bar{\\tau} - \\sum_{m=1}^M w_m \\tilde{\\tau}_m(Z)\\right)^2 \\right] + \\lambda \\text{Penalty}(w)\n",
    "\\end{align}\n",
    "where $\\bar{\\tau}$ is the ATE estimate based on the doubly robust method on the training data and $\\tilde{\\tau}_m(Z)$ are the de-meaned CATE predictions from each model, i.e. $\\tilde{\\tau}_m(Z) = \\tau_m(Z) - E_n[\\tau_m(Z)]$ where the empirical expectation is over the training data.\n",
    "\n",
    "This way we are constructing an ensemble of the CATE on top of the best constant prediction on the training data and penalizing only the offset from this ATE. We avoid adding a coefficient around $\\bar{\\tau}$ to avoid introducing noise to the ATE estimate, due to the smaller sample in the validation set.\n",
    "\n",
    "For the case of the R-loss, this boils down to a penalized linear regression, predicting $\\tilde{Y}$ on the validation set, using $\\tau_1(Z)\\tilde{D}, \\ldots, \\tau_M(Z)\\tilde{D}$ as technical regressors:\n",
    "\\begin{align}\n",
    "\\min_{w} E_n\\left[ \\left(\\tilde{Y} - \\bar{\\tau} \\tilde{D} - \\sum_{m=1}^M w_m \\tilde{\\tau}_m(Z) \\tilde{D}\\right)^2 \\right] + \\lambda \\text{Penalty}(w)\n",
    "\\end{align}\n",
    "where the penalty can also be chosen within the validation set via cross-validation or based on theoretical specifications (e.g. for the Lasso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, LinearRegression\n",
    "\n",
    "F = np.array([m.predict(Zval) for m in models]).T\n",
    "meansF = np.mean(F, axis=0)\n",
    "F = F - meansF\n",
    "\n",
    "stack_models = {}\n",
    "stack_variants = [('OLS', LinearRegression(fit_intercept=False)),\n",
    "                 ('OLS (positive)', LinearRegression(fit_intercept=False, positive=True)),\n",
    "                 ('LassoCV', LassoCV(fit_intercept=False)),\n",
    "                 ('LassoCV (positive)', LassoCV(fit_intercept=False, positive=True)),\n",
    "                 ('RidgeCV', RidgeCV(fit_intercept=False)),]\n",
    "\n",
    "for stackname, stacker in stack_variants:\n",
    "    stacker.fit(F, dr_val - np.mean(dr_preds))\n",
    "    intercept = np.mean(dr_preds) - meansF @ stacker.coef_\n",
    "    stack_models[stackname] = Ensemble(names, models, stacker.coef_, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance(nu, U, y):\n",
    "    n = y.shape[0]\n",
    "    ploss = np.mean((y.reshape(1, -1) - U)**2, axis=1)\n",
    "\n",
    "    def loss(x):\n",
    "        return np.mean((y - U.T @ x)**2)\n",
    "\n",
    "    def qfunction(x):\n",
    "        return (1 - nu) * loss(x) + nu * x @ ploss\n",
    "\n",
    "    def grad_q(x):\n",
    "        return - 2 * (1 - nu) * U @ (y - U.T @ x) / n + nu * ploss\n",
    "\n",
    "    return loss, qfunction, grad_q, ploss\n",
    "\n",
    "def opt(K, qfunction, grad_q):\n",
    "    res = scipy.optimize.minimize(qfunction, np.ones(K)/K, jac=grad_q, bounds=[(0, 1)]*K,\n",
    "                                  constraints=scipy.optimize.LinearConstraint(np.ones((1, K)), lb=1, ub=1),\n",
    "                                  tol=1e-18)\n",
    "    return res.x\n",
    "\n",
    "def qagg(F, y, nu=.5):\n",
    "    scale = max(np.max(np.abs(F)), np.max(np.abs(y)))\n",
    "    loss, qfunction, grad_q, ploss = instance(nu, F.T / scale, y / scale)\n",
    "    return opt(F.shape[1], qfunction, grad_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([m.predict(Zval) for m in models]).T\n",
    "weights = qagg(F, dr_val)\n",
    "stack_models['Q-aggregation'] = Ensemble(names, models, weights, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([m.predict(Zval) for m in models]).T\n",
    "weights = qagg(F, dr_val, nu=0.0)\n",
    "stack_models['OLS (simplex)'] = Ensemble(names, models, weights, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([m.predict(Zval) for m in models]).T\n",
    "weights = qagg(F, dr_val, nu=1.0)\n",
    "stack_models['Best'] = Ensemble(names, models, weights, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(names, stack_models['Q-aggregation'].weights, label='Q-aggregation')\n",
    "plt.plot(names, stack_models['Best'].weights, label='Best')\n",
    "plt.plot(names, stack_models['OLS (simplex)'].weights, label='OLS (simplex)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    stack_ensemble = stack_models['Q-aggregation']\n",
    "    grid = np.unique(np.percentile(X[xfeat], np.arange(0, 102, 2)))\n",
    "    Xpd = pd.DataFrame(np.tile(np.median(X, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                        columns=X.columns)\n",
    "    Xpd[xfeat] = grid\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(Xpd[xfeat], stack_ensemble.predict(Xpd[hetero_feats]),\n",
    "                linewidth=5, color='green',\n",
    "                label=f'Q-aggregate')\n",
    "    plt.plot(Xpd[xfeat], pred_df['mean'], '--', color='orange', label='$Y(\\hat{\\eta})\\sim log(inc)$')\n",
    "    plt.fill_between(Xpd[xfeat], pred_df['mean_ci_lower'], pred_df['mean_ci_upper'], color='orange', alpha=.1)\n",
    "    plt.plot(Xpd[xfeat], pred_df2['mean'], '--', color='blue', label='$Y(\\hat{\\eta})\\sim poly(log(inc), 4)$')\n",
    "    plt.fill_between(Xpd[xfeat], pred_df2['mean_ci_lower'], pred_df2['mean_ci_upper'], color='blue', alpha=.1)\n",
    "    if semi_synth:\n",
    "        plt.plot(Xpd[xfeat], true_cate(Xpd), '--', label='True')\n",
    "        plt.title(f'{score_name}={scorer(stack_ensemble.predict(Zval)):.5f}, '\n",
    "                  f'True RMSE={rmse(true_cate(X), stack_ensemble.predict(Z)):.5f}')\n",
    "    else:\n",
    "        plt.title(f'Q-aggregate CATE Model, '\n",
    "                  f'ATE={np.mean(stack_ensemble.predict(Zval)):.5f}')\n",
    "    plt.xlabel(xfeat)\n",
    "    plt.ylabel('CATE')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{data}-qagg.png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    grid = np.unique(np.percentile(X[xfeat], np.arange(0, 102, 2)))\n",
    "    Xpd = pd.DataFrame(np.tile(np.median(X, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                        columns=X.columns)\n",
    "    Xpd[xfeat] = grid\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for it, (name, model) in enumerate(stack_models.items()):\n",
    "        plt.subplot(4, len(stack_models)//4, it + 1)\n",
    "        preds = model.predict(Xpd[hetero_feats])\n",
    "        plt.plot(Xpd[xfeat], preds, linewidth=5, color='green', label=name)\n",
    "        plt.plot(Xpd[xfeat], pred_df['mean'], '--', color='orange', label='$Y(\\hat{\\eta})\\sim log(inc)$')\n",
    "        plt.fill_between(Xpd[xfeat], pred_df['mean_ci_lower'], pred_df['mean_ci_upper'], color='orange', alpha=.1)\n",
    "        plt.plot(Xpd[xfeat], pred_df2['mean'], '--', color='blue', label='$Y(\\hat{\\eta})\\sim poly(log(inc), 4)$')\n",
    "        plt.fill_between(Xpd[xfeat], pred_df2['mean_ci_lower'], pred_df2['mean_ci_upper'], color='blue', alpha=.1)\n",
    "        if semi_synth:\n",
    "            plt.plot(Xpd[xfeat], true_cate(Xpd), label='True')\n",
    "            plt.title(f'True RMSE={rmse(true_cate(X), model.predict(Z)):.5f}')\n",
    "        else:\n",
    "            plt.title(f'{name}. ATE={np.mean(model.predict(Zval)):.0f}', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.ylim(0, 30000)\n",
    "        plt.xlabel(xfeat, fontsize=14)\n",
    "        plt.ylabel('CATE', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{data}-stacked_ensembles.png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use one of these ensembles as our final best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_best = stack_models['Q-aggregation']\n",
    "# overall_best = softmax_ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Tests on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a selected a winning CATE model (or ensemble), we can run a set of hypothesis tests and other diagnostic metrics on the test set, to see if the model really picked up some dimensions of effect heterogeneity and satisfies some self-conistency checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test Based on Doubly Robust Best-Linear Predictor of CATE using model of CATE\n",
    "\n",
    "If we calculate the doubly robust pseudo outcomes $Y^{DR}(g,p)$ on the test set, using cross-fitting to estimate the models $g,p$, then we know that if the model of the CATE $\\hat{\\tau}$ is good, then the best linear predictor of the true CATE using $(1, \\hat{\\tau}(Z))$ as features should have a statistically significant coefficient on the feature associated with the CATE model. In fact, in an ideal world it should have a coefficient of $1$. Thus we can run such a significance test to measure whether the CATE model $\\tau_m$ has picked up anything signal that is correlated with the true CATE. Note that if $\\theta(X)$ is the true CATE $E[Y(1)-Y(0)\\mid X]$, then the coefficient associated with $\\hat{\\tau}$ in this regression can be interpreted as identifying the quantity:\n",
    "\\begin{align}\n",
    "\\beta_1 := \\frac{\\text{Cov}(\\theta(X), \\hat{\\tau}(Z))}{\\text{Var}(\\hat{\\tau}(Z))} = \\frac{\\text{Cov}(Y(1)-Y(0), \\hat{\\tau}(Z))}{\\text{Var}(\\hat{\\tau}(Z))}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_test = calculate_dr_outcomes(X, D, y, groups, Xtest, Dtest, ytest, groupstest)\n",
    "cate_test = overall_best.predict(Ztest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLS(dr_test, np.stack((np.ones(len(dr_test)), cate_test), axis=-1)).fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Based on Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure whether each group defined by the quartile levels of CATE predictions is consistent with the out-of-sample Group ATE (GATE) for the corresponding group based on the doubly robust GATE estimate. (standard errors here ignore cluster/group correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_val = overall_best.predict(Zval)\n",
    "qs = np.percentile(cate_val, np.arange(0, 101, 25))\n",
    "\n",
    "gate, gate_std, group_prob = np.zeros(len(qs) - 1), np.zeros(len(qs) - 1), np.zeros(len(qs) - 1)\n",
    "predicted_gate = np.zeros(len(qs) - 1)\n",
    "for it in range(len(qs) - 1):\n",
    "    # samples in the [q[it], q[it+1]) quantile group of predicted CATEs\n",
    "    inds = (qs[it] <= cate_test) & (cate_test <= qs[it + 1]) \n",
    "    gate[it] = np.mean(dr_test[inds]) # DR estimate of group average treatment effect (GATE)\n",
    "    gate_std[it] = np.std(dr_test[inds])/np.sqrt(np.sum(inds)) # standard error of GATE\n",
    "    group_prob[it] = np.mean(inds) # probability mass of group\n",
    "    predicted_gate[it] = np.mean(cate_test[inds]) # GATE as calculated from CATE model\n",
    "\n",
    "# weighted average calibration error of cate model\n",
    "cal = np.sum(group_prob * np.abs(gate - predicted_gate))\n",
    "# weighted average calibration error of a constant cate model\n",
    "calbase = np.sum(group_prob * np.abs(gate - np.mean(dr_test)))\n",
    "# calibration score\n",
    "calscore = 1 - cal/calbase\n",
    "plt.title(f'CalScore={calscore:.4f}')\n",
    "plt.errorbar(predicted_gate, gate, yerr=1.96*gate_std, fmt='o')\n",
    "plt.xlabel('Predicted GATE based on CATE model')\n",
    "plt.ylabel('Doubly Robust GATE estimate')\n",
    "plt.savefig(f'{data}-calibration-score.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to interpret what are the differences of characteristics between the top and bottom CATE groups; if we find that they have statistically significantly different GATEs. We can do that by either reporting the mean values of the covariates in the two groups or building some interpretable classification model that distinguishes between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = (qs[0] <= cate_test) & (cate_test < qs[1]) \n",
    "group2 = (qs[-2] <= cate_test) & (cate_test < qs[-1])\n",
    "Ztest1 = Ztest[group1]\n",
    "Ztest2 = Ztest[group2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'group1 means': np.mean(Ztest1, axis=0), \n",
    "                   'group1 s.e.': np.std(Ztest1, axis=0) / np.sqrt(Ztest1.shape[0]),\n",
    "                   'group2 means': np.mean(Ztest2, axis=0),\n",
    "                   'group2 s.e.': np.std(Ztest2, axis=0) / np.sqrt(Ztest2.shape[0]),\n",
    "                   'group1 means - group2 means': np.mean(Ztest1, axis=0) - np.mean(Ztest2, axis=0),\n",
    "                   'diff s.e.': (np.std(Ztest1, axis=0) / np.sqrt(Ztest1.shape[0]) \n",
    "                                 + np.std(Ztest2, axis=0) / np.sqrt(Ztest2.shape[0]))})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "tree.fit(pd.concat((Ztest1, Ztest2)),\n",
    "         np.concatenate((np.zeros(len(Ztest1)), np.ones(len(Ztest2)))))\n",
    "plot_tree(tree, filled=True, feature_names=Ztest1.columns, class_names=['group1', 'group2'])\n",
    "plt.savefig(f'{data}-distill-tree-cate.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Based on Uplift Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These curves are related to \"prioritization\" or \"stratification\" implications of the CATE model. What if we target to treat a $q$-percentage of the population. Then if we trust and follow the CATE model, then we should be offering the treatment to the parts of the population that the CATE model predicts have a CATE larger than the $1-q$-th percentile of the CATE distribution as produced by the CATE.\n",
    "\n",
    "In this case, we might care about the group average treatment effect of the group of people that are treated under such a prioritization rule and how that compares with the overall average treatment effect. Ideally, the group average treatment effect should be larger than the overall average treatment effect if the prioritization rule is correct.\n",
    "\n",
    "Thus if we have a rough estimate $\\hat{\\mu}(q)$ of the $1-q$-quantile $\\mu(q)$ of the CATE predictions distribution we can calculate:\n",
    "\\begin{align}\n",
    "\\tau(q) =~& E[Y(1) - Y(0) \\mid \\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)] - E[Y(1) - Y(0)]\\\\\n",
    "=~& E\\left[(Y(1) - Y(0)) \\frac{1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}}{\\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))}\\right] - E[Y(1) - Y(0)]\n",
    "~=~ \\text{Cov}\\left(Y(1) - Y(0), \\frac{1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}}{\\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))}\\right)\n",
    "\\end{align}\n",
    "We can get such good quantile estimates out-of-sample, by looking at the $1-q$-the empirical quantile of the cate predictions on a sample other than the one that we are using to calculate $\\tau(q)$. For instance, we can use the training/validation samples but not the test samples. This gives us a set of thresholds $\\hat{\\mu}(q_1), \\ldots, \\hat{\\mu}(q_m)$ that are good approximations of the quantile of the predicted CATE distribution. If we target to treat every person with CATE greater than $\\hat{\\mu}(q_t)$ we should expect roughly $q_t$ fraction of the population to be treated.\n",
    "\n",
    "This quantity is identified by replacing the individual effects with the doubly robust pseudo-outcomes:\n",
    "\\begin{align}\n",
    "\\tau(q) =~& E\\left[Y^{DR}(g,p) \\mid \\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\right] - E\\left[Y^{DR}(g,p)\\right]\n",
    "~=~ \\text{Cov}\\left(Y^{DR}(g,p), \\frac{1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}}{\\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))}\\right)\n",
    "\\end{align}\n",
    "\n",
    "We can then plot the curve $\\tau(q)$, understand the Area Under the Curve:\n",
    "\\begin{align}\n",
    "AUTOC := \\int_0^1 \\tau(q) dq\n",
    "\\end{align}\n",
    "and run tests whether that area is positive or that there is any positive point in the curve. These are tests that indicate that the CATE model detected heterogeneity with statistical significance. Moreover, the larger the Area Under the Curve, the better the CATE model is at treatment prioritization. (standard errors here ignore cluster/group correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on out-of-sample CATE thresholds\n",
    "ugrid = np.linspace(5, 95, 50)\n",
    "qs = np.percentile(overall_best.predict(Zval), ugrid)\n",
    "\n",
    "toc, toc_std, group_prob = np.zeros(len(qs)), np.zeros(len(qs)), np.zeros(len(qs))\n",
    "true_toc = np.zeros(len(qs))\n",
    "toc_psi = np.zeros((len(qs), dr_test.shape[0])) # influence function representation of the TOC at each quantile\n",
    "n = len(dr_test)\n",
    "ate = np.mean(dr_test)\n",
    "for it in range(len(qs)):\n",
    "    inds = (qs[it] <= cate_test) # group with larger CATE prediction than the q-th quantile\n",
    "    group_prob = np.sum(inds) / n # fraction of population in this group\n",
    "    toc[it]= np.mean(dr_test[inds]) - ate # tau(q) := E[Y(1) - Y(0) | tau(X) >= q[it]] - E[Y(1) - Y(0)]\n",
    "    # influence function for the tau(q); it is a standard influence function of a \"covariance\"\n",
    "    toc_psi[it, :] = (dr_test - ate) * (inds / group_prob - 1) - toc[it]\n",
    "    toc_std[it] = np.sqrt(np.mean(toc_psi[it]**2) / n) # standard error of tau(q)\n",
    "    if semi_synth:\n",
    "        true_toc[it] = np.mean((true_cate(Xtest) - np.mean(true_cate(Xtest))) * (inds * n / np.sum(inds) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc, yerr=1.96*toc_std, fmt='o', label='Est. TOC')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True TOC')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Average Effect of Treated by CATE over Random\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-toc.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uniform Confidence Band with Multiplier Bootstrap\n",
    "\n",
    "In fact the \"1.96\" is wrong if we want the confidence intervals to hold simultaneoulsy for the whole curve. To have such \"simultaneous coverage\" guarantees we need to calculate a larger \"critical value\" than 1.96. We can calculate the appropriate such constant using the multiplier bootstrap, which tries to estimate the maximum deviation around the mean as a multiple of the standard deviation for each point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For computational reasons if dataset is too large, we should not be constructing\n",
    "# an n_samples x n_bootstrap_samples matrix of multipliers due to memory issues;\n",
    "# even though constructing such a matrix avoids a for loop in python, which is always advised.\n",
    "if dr_test.shape[0] > 1e6:\n",
    "    mboot = np.zeros((len(qs), 1000))\n",
    "    for it in range(1000):\n",
    "        w = np.random.normal(0, 1, size=(dr_test.shape[0],))\n",
    "        mboot[:, it] = (toc_psi / toc_std.reshape(-1, 1)) @ w / n\n",
    "else:\n",
    "    w = np.random.normal(0, 1, size=(dr_test.shape[0], 1000))\n",
    "    mboot = (toc_psi / toc_std.reshape(-1, 1)) @ w / n\n",
    "\n",
    "max_mboot = np.max(np.abs(mboot), axis=0)\n",
    "uniform_critical_value = np.percentile(max_mboot, 95)\n",
    "print(uniform_critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc, yerr=uniform_critical_value*toc_std, fmt='o', label='Est. TOC')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True TOC')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Average Effect of Treated by CATE over Random\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-toc-uniform-band.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if there is any point that is above the zero line, with confidence, in this curve, then the CATE model $\\hat{\\tau}$ has identified heterogeneity in the effect in a statistically significant manner. To do this we need a one-sided confidence interval, as we only care that the quantities are larger than some value with high confidence. We can then calculate the critical value for a uniform one-sided confidence interval across all the points, using the multiplier bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mboot = np.min(mboot, axis=0)\n",
    "uniform_one_side_critical_value = np.abs(np.percentile(min_mboot, 5))\n",
    "print(uniform_one_side_critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc,\n",
    "             yerr=[uniform_one_side_critical_value*toc_std, np.zeros(len(toc))], fmt='o', label='Est. TOC')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True TOC')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Average Effect of Treated by CATE over Random\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-toc-one-sided-band.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Heterogeneity Statistic: {np.max(toc - uniform_one_side_critical_value*toc_std)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calcualte the area under the curve and the confidence interval for that area. If the confidence interval does not contain zero, then we have again detected heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoc_psi = np.sum(toc_psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "autoc = np.sum(toc[:-1] * np.diff(ugrid) / 100)\n",
    "autoc_stderr = np.sqrt(np.mean(autoc_psi**2) / n)\n",
    "print(f'AUTOC: {autoc:.4f}, s.e.: {autoc_stderr:.4f}, '\n",
    "      f'One-Sided 95% CI=[{autoc - scipy.stats.norm.ppf(.95) * autoc_stderr:.4f}, Infty]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qini Curve\n",
    "\n",
    "Similar to the $\\tau(q)$ curve, we can also consider a small variant of that curve that also incorporates the probability of treatment. So instead of looking just at the group average effect of the group of people that are prioritized by the CATE model and subtract the overall effect, we can look at the \"total value\" exctracted by the treated group and contrast it with the total value that would be extracted by an equally large group treated uniformly at random. This corresponds to:\n",
    "\\begin{align}\n",
    "\\tau_{\\text{QINI}}(q) :=~& \\tau(q)\\, \\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))\\\\\n",
    "=~& \\left(E[Y(1) - Y(0)\\mid \\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)] - E[Y(1) - Y(0)]\\right)\\, \\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))\\\\\n",
    "=~& E\\left[ (Y(1) - Y(0)) 1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}\\right] - E[Y(1) - Y(0)]\\, \\Pr(\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q))\\\\\n",
    "=~& E\\left[ (Y(1) - Y(0)) 1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}\\right] - E[Y(1) - Y(0)]\\, E\\left[1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}\\right]\\\\\n",
    "=~& \\text{Cov}\\left(Y(1) - Y(0), 1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}\\right)\n",
    "\\end{align}\n",
    "The latter is approximately $\\tau_{\\text{QINI}}(q) \\approx q \\tau(q)$, but might not be exactly so, due to the inaccuracy of the quantile approximation $\\hat{\\mu}(q)$. \n",
    "\n",
    "The $\\tau_{\\text{QINI}}(q)$ quantity is identified by replacing the individual effects $Y(1)-Y(0)$ with the doubly robust pseudo-outcomes:\n",
    "\\begin{align}\n",
    "\\tau_{\\text{QINI}}(q) := \\text{Cov}\\left(Y^{DR}(g,p), 1\\{\\hat{\\tau}(Z) \\geq \\hat{\\mu}(q)\\}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Similarly, we can look at the area under this curve:\n",
    "\\begin{align}\n",
    "\\text{QINI} := \\int_0^1 \\tau_{\\text{QINI}}(q) dq\n",
    "\\end{align}\n",
    "which is known as the QINI coefficient. (standard errors here ignore cluster/group correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on out-of-sample CATE thresholds\n",
    "ugrid = np.linspace(5, 95, 50)\n",
    "qs = np.percentile(overall_best.predict(Zval), ugrid)\n",
    "\n",
    "toc, toc_std, group_prob = np.zeros(len(qs)), np.zeros(len(qs)), np.zeros(len(qs))\n",
    "true_toc = np.zeros(len(qs))\n",
    "toc_psi = np.zeros((len(qs), dr_test.shape[0]))\n",
    "n = len(dr_test)\n",
    "ate = np.mean(dr_test)\n",
    "for it in range(len(qs)):\n",
    "    inds = (qs[it] <= cate_test) # group with larger CATE prediction than the q-th quantile\n",
    "    group_prob = np.sum(inds) / n # fraction of population in this group\n",
    "    toc[it] = group_prob * (np.mean(dr_test[inds]) - ate) # tau(q) = q * E[Y(1) - Y(0) | tau(X) >= q[it]] - E[Y(1) - Y(0)]\n",
    "    toc_psi[it, :] = (dr_test - ate) * (inds - group_prob) - toc[it] # influence function for the tau(q)\n",
    "    toc_std[it] = np.sqrt(np.mean(toc_psi[it]**2) / n) # standard error of tau(q)\n",
    "    if semi_synth:\n",
    "        true_toc[it] = np.mean((true_cate(Xtest) - np.mean(true_cate(Xtest))) * (inds - group_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc, yerr=1.96*toc_std, fmt='o', label='Est. QINI')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True QINI')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Policy Value over Random Treatment\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-qini.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for uniform coverage we can again use the multiplier bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dr_test.shape[0] > 1e6:\n",
    "    mboot = np.zeros((len(qs), 1000))\n",
    "    for it in range(1000):\n",
    "        w = np.random.normal(0, 1, size=(dr_test.shape[0],))\n",
    "        mboot[:, it] = (toc_psi / toc_std.reshape(-1, 1)) @ w / n\n",
    "else:\n",
    "    w = np.random.normal(0, 1, size=(dr_test.shape[0], 1000))\n",
    "    mboot = (toc_psi / toc_std.reshape(-1, 1)) @ w / n\n",
    "\n",
    "max_mboot = np.max(np.abs(mboot), axis=0)\n",
    "uniform_critical_value = np.percentile(max_mboot, 95)\n",
    "print(uniform_critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc, yerr=uniform_critical_value*toc_std, fmt='o', label='Est. QINI')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True QINI')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Policy Valuee over Random Treatment\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-qini-band.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the one-sided multiplier bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mboot = np.min(mboot, axis=0)\n",
    "uniform_one_side_critical_value = np.abs(np.percentile(min_mboot, 5))\n",
    "print(uniform_one_side_critical_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(100 - ugrid, toc, yerr=[uniform_one_side_critical_value*toc_std, np.zeros(len(toc))],\n",
    "             fmt='o', label='Est. QINI')\n",
    "plt.plot(100 - ugrid, np.zeros(len(ugrid)))\n",
    "if semi_synth:\n",
    "    plt.plot(100 - ugrid, true_toc, 'o', label='True QINI')\n",
    "plt.xlabel(\"Percentage treated\")\n",
    "plt.ylabel(\"Gain in Average Effect of Treated by CATE over Random\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{data}-qini-one-sided-band.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Heterogeneity Statistic: {np.max(toc - uniform_one_side_critical_value*toc_std)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qini_psi = np.sum(toc_psi[:-1] * np.diff(ugrid).reshape(-1, 1) / 100, 0)\n",
    "qini = np.sum(toc[:-1] * np.diff(ugrid) / 100)\n",
    "qini_stderr = np.sqrt(np.mean(qini_psi**2) / n)\n",
    "print(f'QINI: {qini:.4f}, s.e.: {qini_stderr:.4f}, '\n",
    "      f'One-Sided 95% CI=[{qini - scipy.stats.norm.ppf(.95) * qini_stderr:.4f}, Infty]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals on CATE Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move on to the subject of constructing confidence intervals for the predictions of CATE models. Confidence intervals for CATE predictions is an inherently harder task. In its generality it is at least as hard as constructing confidence intervals for the predictions of a non-parametric regression function; which is a statistically daunting task. \n",
    "\n",
    "Two approaches help here:\n",
    "- the first is to relax our goals and only go after the best linear approximation of the CATE on some pre-defined set of engineered features. This is the approach taken by the DRLearner with a linear final CATE model. We have already seen this approach earlier in this notebook and how we can even construct simultaneous (joint) confidence intervals for CATE parameters and for CATE predictions.\n",
    "- the second is to use more data-adaptive approaches like random forests to side step the curse of dimensionality and potentially adapt to sparsity in the regression function (though theoretically such an adaptivity is in the worst case imposssible; it tends to work well in practice. This is the approach taken by CausalForests or Doubly Robust Forests that are both based on the idea of Generalized Random Forests, which is an extension of classical forests for solving problems defined via conditional moment restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Confidence Intervals with Causal Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(standard errors here ignore cluster/group correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import CausalForest\n",
    "\n",
    "yres = y - res_preds\n",
    "Dres = D - prop_preds\n",
    "cf = CausalForest(4000, criterion='het', max_depth=5, max_samples=.4,\n",
    "                  min_samples_leaf=50, min_weight_fraction_leaf=.0, random_state=random_seed)\n",
    "cf.fit(Z, Dres, yres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat = np.argsort(cf.feature_importances_)[-1]\n",
    "print(Z.columns[top_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.unique(np.percentile(Z.iloc[:, top_feat], np.arange(0, 105, 5)))\n",
    "Zpd = pd.DataFrame(np.tile(np.median(Z, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=Z.columns)\n",
    "Zpd.iloc[:, top_feat] = grid\n",
    "\n",
    "preds, lb, ub = cf.predict(Zpd, interval=True, alpha=.1)\n",
    "preds = preds.flatten()\n",
    "lb = lb.flatten()\n",
    "ub = ub.flatten()\n",
    "plt.errorbar(Zpd.iloc[:, top_feat], preds, yerr=(preds-lb, ub-preds))\n",
    "plt.xlabel(Zpd.columns[top_feat])\n",
    "plt.ylabel('Predicted CATE (at median value of other features)')\n",
    "plt.savefig(f'{data}-causal-forest.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if semi_synth:\n",
    "    true_proj = true_cate(X)\n",
    "    preds, lb, ub = cf.predict(Z, interval=True, alpha=.1)\n",
    "    preds = preds.flatten()\n",
    "    lb = lb.flatten()\n",
    "    ub = ub.flatten()\n",
    "    inds = np.argsort(true_proj)\n",
    "    plt.plot(true_proj[inds], preds[inds])\n",
    "    plt.fill_between(true_proj[inds], lb[inds].flatten(), ub[inds].flatten(), alpha=.4)\n",
    "    plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "             np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "    plt.xlabel('True CATE')\n",
    "    plt.ylabel('Predicted CATE')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Parametric Confidence Intervals with Doubly Robust Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(standard errors here ignore cluster/group correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.grf import RegressionForest\n",
    "\n",
    "drrf = RegressionForest(4000, max_depth=5, max_samples=.4, min_samples_leaf=50,\n",
    "                       min_weight_fraction_leaf=.0, random_state=random_seed)\n",
    "drrf.fit(Z, dr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feat = np.argsort(drrf.feature_importances_)[-1]\n",
    "print(Z.columns[top_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.unique(np.percentile(Z.iloc[:, top_feat], np.arange(0, 105, 5)))\n",
    "Zpd = pd.DataFrame(np.tile(np.median(Z, axis=0, keepdims=True), (len(grid), 1)),\n",
    "                    columns=Z.columns)\n",
    "Zpd.iloc[:, top_feat] = grid\n",
    "\n",
    "preds, lb, ub = drrf.predict(Zpd, interval=True, alpha=.1)\n",
    "preds = preds.flatten()\n",
    "lb = lb.flatten()\n",
    "ub = ub.flatten()\n",
    "plt.errorbar(Zpd.iloc[:, top_feat], preds, yerr=(preds-lb, ub-preds))\n",
    "plt.xlabel(Zpd.columns[top_feat])\n",
    "plt.ylabel('Predicted CATE (at median value of other features)')\n",
    "plt.savefig(f'{data}-doubly-robust-forest.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if semi_synth:\n",
    "    true_proj = true_cate(X)\n",
    "    preds, lb, ub = drrf.predict(Z, interval=True, alpha=.1)\n",
    "    preds = preds.flatten()\n",
    "    lb = lb.flatten()\n",
    "    ub = ub.flatten()\n",
    "    inds = np.argsort(true_proj)\n",
    "    plt.plot(true_proj[inds], preds[inds])\n",
    "    plt.fill_between(true_proj[inds], lb[inds].flatten(), ub[inds].flatten(), alpha=.4)\n",
    "    plt.plot(np.linspace(np.min(true_proj), np.max(true_proj), 100),\n",
    "             np.linspace(np.min(true_proj), np.max(true_proj), 100))\n",
    "    plt.xlabel('True CATE')\n",
    "    plt.ylabel('Predicted CATE')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose our goal is to estimate the best treatment policy $\\pi: Z \\to \\{0, 1\\}$. The policy gains over no treatment for any policy $\\pi$ can be identified as:\n",
    "\\begin{align}\n",
    "V(\\pi) := E[\\pi(Z)\\, (Y(1) - Y(0))] = E\\left[\\pi(Z)\\, Y^{DR}(g,p)\\right]\n",
    "\\end{align}\n",
    "\n",
    "Moreover, note that optimizing the linear objective $E[\\pi(Z)\\, Y^{DR}(g,p)]$ can also be written as a sample-weighted classification problem, where the goal of $\\pi$ is to much the sign of $Y^{DR}(g,p)$, with sample weights $|Y^{DR}(g,p)|$.\n",
    "\\begin{align}\n",
    "\\text{argmin}_{\\pi} V(\\pi) =~& \\text{argmin}_{\\pi} E\\left[(2\\pi(Z) - 1)\\, Y^{DR}(g,p)\\right]\\\\\n",
    "=~& \\text{argmin}_{\\pi} E\\left[\\left(2\\pi(Z) - 1\\right)\\, \\text{sign}\\left(Y^{DR}(g,p)\\right)\\, \\left|Y^{DR}(g,p)\\right|\\right] \\\\\n",
    "=~& \\text{argmin}_{\\pi} E\\left[1\\left\\{2\\pi(Z) - 1 = \\text{sign}\\left(Y^{DR}(g,p)\\right)\\right\\} \\left|Y^{DR}(g,p)\\right|\\right] \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DecisionTreeClassifier(max_depth=2, min_impurity_decrease=1e-3, random_state=random_seed)\n",
    "policy.fit(Z, np.sign(dr_preds), sample_weight=np.abs(dr_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the tree to visualize the partitions and the treatment recommendations. The details that are displayed on each node are also useful in understanding the group average treatment effect for each node. In particular, the information `samples=N`, gives us the size of each node $N$, and the information `value=[A, B]`, then `A` is the sum of the $|Y^{DR}(g,p)|$ for the samples where $Y^{DR}(g,p)<0$ and similarly, `B` is the sum of $|Y^{DR}(g,p)|$ for the samples where $Y^{DR}(g,p)>0$. Thus to get the GATE for each node, we simply do `(B-A)/N`, which would correspond to $\\frac{1}{N}\\sum_{i\\in \\text{node}} Y^{DR}(g,p)$, which is the doubly robust estimate of the GATE for the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_tree(policy, filled=True, feature_names=Z.columns,\n",
    "          class_names=['Negative', 'Positive'])\n",
    "plt.savefig(f'{data}-policy-tree.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare with the optimal tree policy if we had access to the true treatment CATE values for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cates = true_cate(X)\n",
    "oracle_policy = DecisionTreeClassifier(max_depth=2, min_impurity_decrease=1e-3, random_state=random_seed)\n",
    "oracle_policy.fit(Z, np.sign(true_cates), sample_weight=np.abs(true_cates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plot_tree(oracle_policy, filled=True, feature_names=Z.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
